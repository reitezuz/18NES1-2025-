{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMY6K2OmOwXxoczMkxVMU85",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/reitezuz/18NES1-2025-/blob/main/week2/perceptron_library.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Perceptron library"
      ],
      "metadata": {
        "id": "aZMeecY72EOa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gqOD3Yp20fSo",
        "outputId": "2045c8b3-865f-4171-da13-ce9eddcfb7a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing perceptron.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile perceptron.py\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "class Perceptron:\n",
        "    def __init__(self, weights, bias, binary=True):\n",
        "        self.weights = np.array(weights, dtype=float)\n",
        "        self.bias = bias\n",
        "        self.binary = binary\n",
        "\n",
        "    def __str__(self):\n",
        "        return f\"Perceptron(weights={self.weights}, bias={self.bias}, binary={self.binary})\"\n",
        "\n",
        "    def activation(self, x):\n",
        "        if self.binary:\n",
        "            return np.where(x > 0, 1, np.where(x < 0, 0, 0.5))\n",
        "        else:\n",
        "            return np.where(x > 0, 1, np.where(x < 0, -1, 0))\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        potential = np.dot(inputs, self.weights) + self.bias  #Corrected dot product\n",
        "        return self.activation(potential)\n",
        "\n",
        "    def rosenblatt_batch(self, training_inputs, true_outputs, learning_rate = 1, epochs=100, print_progress=True):\n",
        "        if epochs > 1:\n",
        "            print(\"Rosenblatt batch:\")\n",
        "        else:\n",
        "            print(\"Hebbian\")\n",
        "        for epoch in range(epochs):\n",
        "            predictions = self.forward(training_inputs)\n",
        "            errors = sign(true_outputs - predictions)\n",
        "\n",
        "            # Batch update\n",
        "            # self.weights += np.sum(errors * training_inputs, axis=0)\n",
        "            self.weights += learning_rate * training_inputs.T @ errors # more efficient: transposition and matrix multiplication\n",
        "            self.bias += learning_rate * np.sum(errors)\n",
        "            if print_progress:\n",
        "                print(f\"Epoch: {epoch} Weights: {self.weights} Bias: {self.bias} {np.sum(np.abs(errors))}\")\n",
        "            if np.sum(np.abs(errors)) == 0:\n",
        "                break\n",
        "        print(f\"The training ended after {epoch+1} epochs.\")\n",
        "\n",
        "    def hebbian(self, training_inputs, true_outputs):\n",
        "        return self.rosenblatt_batch(training_inputs, true_outputs, 1)\n",
        "\n",
        "    def rosenblatt_iterative(self, training_inputs, true_outputs, learning_rate = 1, epochs=100, print_progress=True):\n",
        "        print(\"Rosenblatt iterative:\")\n",
        "        for epoch in range(epochs):\n",
        "\n",
        "            # Shuffle the training data for each epoch\n",
        "            shuffled_indices = np.random.permutation(len(training_inputs))\n",
        "            training_inputs = training_inputs[shuffled_indices]\n",
        "            true_outputs = true_outputs[shuffled_indices]\n",
        "\n",
        "            errors = 0\n",
        "            for inputs, true_output in zip(training_inputs, true_outputs):\n",
        "                prediction = self.forward(inputs)\n",
        "                error = sign(true_output - prediction)\n",
        "                if abs(error) and print_progress:\n",
        "                    print(f\"Epoch: {epoch}, Inputs: {inputs}, Prediction: {prediction} Old weights: {self.weights} {self.bias} New weights: {self.weights+learning_rate*error*inputs} {self.bias+learning_rate*error}\")\n",
        "                self.weights += learning_rate * error * inputs\n",
        "                self.bias += learning_rate * error\n",
        "                errors += abs(error) # Accumulate the absolute errors\n",
        "            if errors == 0: # Check if there were any errors in this epoch.\n",
        "                break # End training if no errors were found\n",
        "        print(f\"The training ended after {epoch+1} epochs.\")\n",
        "\n",
        "    def rosenblatt_iterative_best(self, training_inputs, true_outputs, learning_rate = 1, epochs=100, print_progress=True):\n",
        "        print(\"Rosenblatt iterative + store best solution:\")\n",
        "        min_errors = float('inf')\n",
        "        best_epoch = -1\n",
        "        for epoch in range(epochs):\n",
        "\n",
        "            # Shuffle the training data for each epoch\n",
        "            shuffled_indices = np.random.permutation(len(training_inputs))\n",
        "            training_inputs = training_inputs[shuffled_indices]\n",
        "            true_outputs = true_outputs[shuffled_indices]\n",
        "\n",
        "            errors = 0\n",
        "            for inputs, true_output in zip(training_inputs, true_outputs):\n",
        "                prediction = self.forward(inputs)\n",
        "                error = sign(true_output - prediction)\n",
        "                if abs(error) and print_progress:\n",
        "                    print(f\"Epoch: {epoch}, Inputs: {inputs}, Prediction: {prediction} Old weights: {self.weights} {self.bias} New weights: {self.weights+learning_rate*error*inputs} {self.bias+learning_rate*error}\")\n",
        "                self.weights += learning_rate * error * inputs\n",
        "                self.bias += learning_rate * error\n",
        "                errors += abs(error) # Accumulate the absolute errors\n",
        "            if errors < min_errors:\n",
        "                min_errors = errors\n",
        "                best_weights = self.weights.copy()\n",
        "                best_bias = self.bias\n",
        "                best_epoch = epoch\n",
        "            if errors == 0: # Check if there were any errors in this epoch.\n",
        "                break # End training if no errors were found\n",
        "        self.weights = best_weights\n",
        "        self.bias = best_bias\n",
        "        print(f\"The best solution found in {best_epoch+1} epochs with error {min_errors}.\")\n",
        "        print(f\"The training ended after {epoch+1} epochs.\")\n",
        "\n",
        "\n",
        "def plot_decision_boundary_2D(perceptron, training_inputs, true_outputs):\n",
        "    \"\"\"Plots the decision boundary of the perceptron.\"\"\"\n",
        "\n",
        "    # Extract the first two columns of training_inputs\n",
        "    x1 = training_inputs[:, 0]\n",
        "    x2 = training_inputs[:, 1]\n",
        "\n",
        "    # Generate points for visualization\n",
        "    x = np.linspace(np.min(x1), np.max(x1), 100)\n",
        "    y = -(perceptron.weights[0] * x + perceptron.bias) / perceptron.weights[1]\n",
        "\n",
        "    # Plot the decision boundary\n",
        "    plt.plot(x, y, label='Decision Boundary')\n",
        "\n",
        "    # Plot the points (optional)\n",
        "    vals = [0, 1] if perceptron.binary else [-1, 1]\n",
        "    for i in vals:\n",
        "        for j in vals:\n",
        "            out = perceptron.forward(np.array([i, j]))\n",
        "            if out == 1:\n",
        "                plt.scatter(i, j, color='green')\n",
        "            else:\n",
        "                plt.scatter(i, j, color='red')\n",
        "\n",
        "    plt.xlabel('x1', fontsize=12)\n",
        "    plt.ylabel('x2', fontsize=12)\n",
        "    plt.title('Perceptron Decision Boundary', fontsize=14)\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "\n",
        "    # Highlight x and y axes\n",
        "    plt.axhline(0, color='black', linewidth=0.8)  # x-axis\n",
        "    plt.axvline(0, color='black', linewidth=0.8)  # y-axis\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "def plot_decision_boundary_3D(perceptron, training_inputs, true_outputs):\n",
        "    \"\"\"Plots the decision boundary of the perceptron in 3D.\"\"\"\n",
        "    # Select first three columns if more than three exist\n",
        "    training_inputs = training_inputs[:, :3]\n",
        "\n",
        "    # Check if the input data has 3 features\n",
        "    if training_inputs.shape[1] != 3:\n",
        "        raise ValueError(\"Input data must have at least 3 features for 3D visualization.\")\n",
        "\n",
        "    # Create a meshgrid of points\n",
        "    x_min, x_max = training_inputs[:, 0].min() - 1, training_inputs[:, 0].max() + 1\n",
        "    y_min, y_max = training_inputs[:, 1].min() - 1, training_inputs[:, 1].max() + 1\n",
        "    z_min, z_max = training_inputs[:, 2].min() - 1, training_inputs[:, 2].max() + 1\n",
        "\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),\n",
        "                          np.arange(y_min, y_max, 0.1))\n",
        "\n",
        "    # Calculate z values for the decision boundary\n",
        "    zz = (-perceptron.weights[0] * xx - perceptron.weights[1] * yy - perceptron.bias) / perceptron.weights[2]\n",
        "\n",
        "    # Create the 3D plot\n",
        "    fig = plt.figure()\n",
        "    ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "    # Plot the decision boundary\n",
        "    ax.plot_surface(xx, yy, zz, alpha=0.5)\n",
        "\n",
        "    # Plot the training data points\n",
        "    ax.scatter(training_inputs[:, 0], training_inputs[:, 1], training_inputs[:, 2], c=true_outputs, cmap=plt.cm.Paired)\n",
        "\n",
        "    ax.set_xlabel('x1')\n",
        "    ax.set_ylabel('x2')\n",
        "    ax.set_zlabel('x3')\n",
        "    ax.set_title('Perceptron Decision Boundary (3D)')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "def sign(x):\n",
        "    return np.where(x > 0, 1, np.where(x < 0, -1, 0))\n",
        "\n",
        "def perceptron_error(true_outputs, predicted_outputs):\n",
        "    return np.sum(true_outputs != predicted_outputs)\n",
        "    #return np.mean(true_outputs != predicted_outputs)"
      ]
    }
  ]
}
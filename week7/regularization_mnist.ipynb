{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/reitezuz/18NES1-2025-/blob/main/week7/regularization_mnist.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FfX7M9hjkWsC"
      },
      "source": [
        "# MLP and generalization - classifying digits from the MNIST dataset\n",
        "\n",
        "Inspired by: https://github.com/fchollet/deep-learning-with-python-notebooks/blob/master/chapter02_mathematical-building-blocks.ipynb  \n",
        "\n",
        "MNIST dataset is a dataset of handwritten digits. It contains a training set of 60000 greyscale 28x28 images and a testing set of 10000 images of digits written by different people.\n",
        "\n",
        "https://yann.lecun.com/exdb/mnist/\n",
        "\n",
        "https://en.wikipedia.org/wiki/MNIST_database\n",
        "\n",
        "\n",
        "For further reference datasets for deep learning, investigate:\n",
        "https://keras.io/api/datasets/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MzwRu2_8ViC7"
      },
      "source": [
        "## Load, observe and analyze the data\n",
        "\n",
        "- to observe overtraining, we use just a smaller subset of the original dataset for training:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MwVsc3VekVh7"
      },
      "outputs": [],
      "source": [
        "number_of_samples = 100\n",
        "number_of_test_samples = 5000\n",
        "\n",
        "# Load the MNIST dataset\n",
        "import keras\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "\n",
        "# Load the MNIST dataset\n",
        "from keras.datasets import mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "\n",
        "# Shuffle training data\n",
        "train_images, train_labels = shuffle(train_images, train_labels, random_state=42)\n",
        "\n",
        "# Select a small random subset (e.g. 1000 samples)\n",
        "train_images = train_images[:number_of_samples]\n",
        "train_labels = train_labels[:number_of_samples]\n",
        "\n",
        "# Shuffle the testing data\n",
        "test_images, test_labels = shuffle(test_images, test_labels, random_state=42)\n",
        "test_images = test_images[:number_of_test_samples]\n",
        "test_labels = test_labels[:number_of_test_samples]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2hrCLc9IViC_"
      },
      "source": [
        "Observe the data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KvvntzS9lg7M"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# training samples - images 28x28 in greyscale\n",
        "print(train_images.shape, train_labels.shape)\n",
        "\n",
        "# testing samples - images 28x28 in greyscale\n",
        "print(test_images.shape, test_labels.shape)\n",
        "\n",
        "# 10 categories\n",
        "print(len(train_labels), train_labels[:10], np.min(train_labels), np.max(train_labels))\n",
        "\n",
        "print(\"Extremes of pixel values:\", np.min(train_images), np.max(train_images))\n",
        "\n",
        "# Distribution of training and testing labels\n",
        "print(\"Label distribution:\", np.bincount(train_labels))\n",
        "print(\"Label distribution:\", np.bincount(test_labels))\n",
        "\n",
        "# Check for missing values\n",
        "print(\"Number of missing values in training images:\", np.sum(np.isnan(train_images)))\n",
        "print(\"Number of missing values in test images:\", np.sum(np.isnan(test_images)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PHr6LuBMomDO"
      },
      "outputs": [],
      "source": [
        "# Display some images\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Display the first 10 images\n",
        "plt.figure(figsize=(10, 10))\n",
        "for i in range(10):\n",
        "    plt.subplot(5, 5, i + 1)\n",
        "    plt.xticks([])  # Remove axis ticks and the grids\n",
        "    plt.yticks([])\n",
        "    plt.grid(False)\n",
        "    plt.imshow(train_images[i], cmap=plt.cm.binary) # Display the current image using a binary color map\n",
        "    plt.xlabel(train_labels[i])\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0YaqVCUJViDB"
      },
      "source": [
        "**Observation:**\n",
        "1. The letters are well centered and similar in size, no values ​​are missing, there are no missing values -> we need no data augmentation\n",
        "2. The input data have the form of 3D-tensor.\n",
        "3. The pixels have values 0...255."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHxfENUMrEQi"
      },
      "source": [
        "## Preprocess the data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cwVnJ6fdrDgr"
      },
      "outputs": [],
      "source": [
        "# Set one-hot encoding of the labels:\n",
        "do_one_hot_encoding = False\n",
        "do_tanh = False\n",
        "\n",
        "import keras\n",
        "# 1. Reshape and normalize the data:\n",
        "# reshape the data into a flat vector (784 elements) for input to our MLP neural network.\n",
        "x_train_0 = train_images.reshape(number_of_samples, 28 * 28)\n",
        "x_test_0 = test_images.reshape(number_of_test_samples, 28 * 28)\n",
        "y_train = train_labels\n",
        "y_test = test_labels\n",
        "\n",
        "# 2. Convert the pixel values from integers [0-255] to floating-point numbers and normalize them to the range [0, 1].\n",
        "x_train = x_train_0.astype('float32') / 255\n",
        "x_test = x_test_0.astype('float32') / 255\n",
        "\n",
        "# Alternatively (for tanh): [-1, 1]\n",
        "if do_tanh:\n",
        "    x_train = x_train *2  - 1\n",
        "    x_test = x_test *2  - 1\n",
        "\n",
        "# 3. Arbitrary: one-hot encode the labels:\n",
        "# For example, the label 3 would become [0, 0, 0, 1, 0, 0, 0, 0, 0, 0].\n",
        "y_train_categorical = keras.utils.to_categorical(y_train, num_classes=10)\n",
        "y_test_categorical  = keras.utils.to_categorical(y_test, num_classes=10)\n",
        "\n",
        "# print(x_train[1])\n",
        "print(y_train[:3])\n",
        "print(y_train_categorical[:3])\n",
        "\n",
        "# For one-hot encoding:\n",
        "if do_one_hot_encoding:\n",
        "    y_train = y_train_categorical\n",
        "    y_test = y_test_categorical\n",
        "\n",
        "# 4. Split the training data into training and validation sets\n",
        "# The validation set is used to monitor the performance of the model during training and prevent overfitting.\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UiItbng2zUM3"
      },
      "source": [
        "## MLP model for multiclass classification:\n",
        "- 'softmax' activation function in the output layer\n",
        "- 'relu' or 'tanh' in the hidden layers\n",
        "- if labels are one-hot vectors:\n",
        "    CategoricalCrossentropy  loss function and CategoricalAccuracy metrics\n",
        "\n",
        "- if labels are provided as integers:\n",
        "     SparseCategoricalCrossentropy  loss function and SparseCategoricalAccuracy metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model architecture:\n",
        "from keras import layers\n",
        "model = keras.Sequential([\n",
        "    layers.InputLayer(shape=(28 * 28,)),    # Input layer\n",
        "    layers.Dense(64, activation='relu'),   # First hidden layer  # kernel_initializer='he_normal', bias_initializer='zeros')\n",
        "    layers.Dense(64, activation='relu'),    # Second hidden layer\n",
        "    layers.Dense(10, activation='softmax')  # Output layer for multiclass classification\n",
        "])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "# Configure the model:\n",
        "if do_one_hot_encoding:\n",
        "    model.compile(optimizer=keras.optimizers.SGD(learning_rate = 0.001), # Adam, RMSProp\n",
        "                  loss= keras.losses.CategoricalCrossentropy(),\n",
        "                  metrics= [keras.metrics.CategoricalAccuracy(\"accuracy\")])\n",
        "else:\n",
        "    model.compile(optimizer=keras.optimizers.SGD(learning_rate = 0.001), # Adam, RMSProp\n",
        "                  loss= keras.losses.SparseCategoricalCrossentropy(),\n",
        "                  metrics= [keras.metrics.SparseCategoricalAccuracy(\"accuracy\")])"
      ],
      "metadata": {
        "id": "QcCD7Pt2RT-3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define and train the model"
      ],
      "metadata": {
        "id": "6FMVQdavR91E"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uiS5smsrViDJ"
      },
      "outputs": [],
      "source": [
        "# Data frame for results\n",
        "import pandas as pd\n",
        "\n",
        "columns = [\"Model Name\", \"Test Accuracy\", \"Test Loss\", \"Train Accuracy\", \"Train Loss\", \"Time (s)\", \"Epochs\", \"Details\"]\n",
        "results_df = pd.DataFrame(columns=columns)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# plot the training progress:\n",
        "def plot_history(history):\n",
        "    history_dict = history.history\n",
        "    print(history_dict.keys())\n",
        "\n",
        "    from matplotlib import pyplot as plt\n",
        "\n",
        "    # Plot training & validation accuracy values\n",
        "    plt.plot(history.history['accuracy'])\n",
        "    plt.plot(history.history['val_accuracy'])\n",
        "    plt.title('Model accuracy')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "    plt.show()\n",
        "\n",
        "    # Plot training & validation loss values\n",
        "    plt.plot(history.history['loss'])\n",
        "    plt.plot(history.history['val_loss'])\n",
        "    plt.title('Model loss')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "    plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "Yispa2xCfm-w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set some of the hyperparameters:\n",
        "do_early_stopping = False\n",
        "max_epochs = 50\n",
        "batch_size = 128\n",
        "num_neur_1 = 512 # number of neurons in the first hidden layer\n",
        "num_neur_2 = 256 # number of neurons in the second hidden layer\n",
        "hidden_activation = 'relu'\n",
        "do_tensorboard = True\n",
        "details = \"\" # your comment\n",
        "# already set: do_tanh, do_one_hot_encoding\n",
        "# further hyperparameters: optimizer, loss, metrics,...\n",
        "\n",
        "###############################################\n",
        "# Define the model architecture\n",
        "from keras import layers\n",
        "model = keras.Sequential([\n",
        "    layers.InputLayer(shape=(28 * 28,)),    # Input layer\n",
        "    layers.Dense(num_neur_1, activation=hidden_activation), # First hidden layer   # , kernel_initializer='he_normal', bias_initializer='zeros')\n",
        "    layers.Dense(num_neur_2, activation=hidden_activation), # Second hidden layer\n",
        "    # layers.Dense(num_neur_3, activation=hidden_activation), # Third hidden layer\n",
        "    layers.Dense(10, activation='softmax')  # Output layer for multiclass classification\n",
        "])\n",
        "import datetime\n",
        "model_name = \"mnist_mlp_\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") + '.keras'\n",
        "model.summary()\n",
        "\n",
        "# Configure the model:\n",
        "optimizer = keras.optimizers.Adam() #SGD(learning_rate = 0.1) # SGD, Adam, RMSProp\n",
        "if do_one_hot_encoding:\n",
        "    model.compile(optimizer=optimizer,\n",
        "                  loss= keras.losses.CategoricalCrossentropy(),\n",
        "                  metrics= [keras.metrics.CategoricalAccuracy(\"accuracy\")])\n",
        "else:\n",
        "    model.compile(optimizer=optimizer,\n",
        "                  loss= keras.losses.SparseCategoricalCrossentropy(),\n",
        "                  metrics= [keras.metrics.SparseCategoricalAccuracy(\"accuracy\")])\n",
        "\n",
        "###############################################\n",
        "# Define callbacks (e.g., early stopping):\n",
        "callbacks = []\n",
        "if do_early_stopping:\n",
        "    from keras.callbacks import EarlyStopping\n",
        "    early_stopping = EarlyStopping(monitor=\"val_loss\", patience=3, restore_best_weights=True)\n",
        "    callbacks.append(early_stopping)\n",
        "if do_tensorboard:\n",
        "    from keras.callbacks import TensorBoard\n",
        "    tensorboard_callback = TensorBoard(log_dir=\"./logs_mnist/\"+model_name, histogram_freq=1, write_steps_per_second=True)\n",
        "    callbacks.append(tensorboard_callback)\n",
        "\n",
        "################################################\n",
        "# Train the model\n",
        "import time\n",
        "start_time = time.time()\n",
        "history = model.fit(x_train, y_train, epochs=max_epochs, batch_size=batch_size, validation_data=(x_val, y_val),\n",
        "                    callbacks=callbacks)\n",
        "time_fit = time.time() - start_time\n",
        "\n",
        "###############################\n",
        "# Plot the training progress:\n",
        "plot_history(history)\n",
        "\n",
        "# Evaluate the model on the training, validation and test sets\n",
        "train_loss, train_acc = model.evaluate(x_train, y_train)\n",
        "val_loss, val_acc = model.evaluate(x_val, y_val)\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "\n",
        "print('Training accuracy:', train_acc, '\\nTrain loss:', train_loss)\n",
        "print('Validation accuracy:', val_acc, '\\nVal loss:', val_loss)\n",
        "print('Test accuracy:', test_acc, '\\nTest loss:', test_loss)\n",
        "\n",
        "###############################\n",
        "# Save the model:\n",
        "import os\n",
        "model_dir = \"./models/\"\n",
        "if not os.path.exists(model_dir):\n",
        "    os.makedirs(model_dir)\n",
        "model.save(model_dir + model_name)\n",
        "\n",
        "#################################\n",
        "# Add results to the dataframe:\n",
        "model_details = f\"{num_neur_1}-{num_neur_2}-{hidden_activation}-{do_early_stopping}-ep.:{max_epochs}-bs:{batch_size} {details}\"\n",
        "new_entry = {\n",
        "    \"Model Name\" : model_name,\n",
        "    \"Details\" : model_details,\n",
        "    \"Test Accuracy\" : test_acc,\n",
        "    \"Test Loss\" : test_loss,\n",
        "    \"Train Accuracy\" : train_acc,\n",
        "    \"Train Loss\" : train_loss,\n",
        "    \"Time (s)\" : time_fit,\n",
        "    \"Epochs\" : len(history.epoch),\n",
        "}\n",
        "results_df = pd.concat([results_df, pd.DataFrame([new_entry])], ignore_index=True)\n",
        "# View and and save the dataframe:\n",
        "results_df.to_csv(model_dir + \"mnist_results.csv\", index=False)\n",
        "print(\"Results:\")\n",
        "print(results_df)"
      ],
      "metadata": {
        "id": "AJ-oCcr4T4Xc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_df"
      ],
      "metadata": {
        "id": "CJBnkI59jgvN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9oYaO_138aa"
      },
      "source": [
        "## Evaluate the model and make predictions on new data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vNHkFYxs_K9T"
      },
      "outputs": [],
      "source": [
        "# Get predicted probabilities for the test set\n",
        "y_pred_probs = model.predict(x_test)\n",
        "print(y_pred_probs[0])\n",
        "\n",
        "# Get the predicted class for each sample\n",
        "y_pred = np.argmax(y_pred_probs, axis=1)\n",
        "\n",
        "print(\"Predicted labels:\", y_pred[:10])\n",
        "print(\"True labels:\", y_test[:10])\n",
        "\n",
        "# Misclassified indices:\n",
        "misclassified_indices = np.where(y_pred != y_test)[0]\n",
        "num_misclassified = len(misclassified_indices)\n",
        "print(\"Number of misclassified images:\", num_misclassified,\n",
        "      \"out of\", len(y_test), \", accuracy\", test_acc)\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Plot the confusion matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n",
        "\n",
        "# Plot some misclassified images\n",
        "num_images_to_plot = 5\n",
        "plt.figure(figsize=(10, 10))\n",
        "for i in range(min(num_images_to_plot, len(misclassified_indices))):\n",
        "    index = misclassified_indices[i]\n",
        "    plt.subplot(1, num_images_to_plot, i + 1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.grid(False)\n",
        "    plt.imshow(test_images[index], cmap=plt.cm.binary)\n",
        "    plt.xlabel(f\"Pred: {y_pred[index]}, True: {y_test[index]}\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v6XQlpZb__0M"
      },
      "outputs": [],
      "source": [
        "# Plot some misclassified images from a given target (or predicted) class\n",
        "target_class = 3\n",
        "misclassified_indices_class = np.where((y_pred != y_test) & (y_test == target_class))[0]\n",
        "#misclassified_indices_class = np.where((y_pred != y_test) & (y_pred == target_class))[0]\n",
        "\n",
        "\n",
        "\n",
        "# Display the first 25 misclassified images for the target class\n",
        "plt.figure(figsize=(10, 10))\n",
        "for i in range(min(25, len(misclassified_indices_class))):\n",
        "    index = misclassified_indices_class[i]\n",
        "    plt.subplot(5, 5, i + 1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.grid(False)\n",
        "    plt.imshow(test_images[index], cmap=plt.cm.binary)\n",
        "    plt.xlabel(f\"True:{y_test[index]}, Pred:{y_pred[index]}\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_eAc9Q0WViDP"
      },
      "outputs": [],
      "source": [
        "###############################################\n",
        "# Load TensorBoard notebook extension\n",
        "%load_ext tensorboard\n",
        "\n",
        "# Start TensorBoard before training begins\n",
        "%tensorboard --logdir logs_mnist --reload_interval=1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xn-GJqLSQ-dO"
      },
      "source": [
        "# Exercises\n",
        "\n",
        "1. **Experiment with the size of the training dataset and the size of the model.**  \n",
        "   Observe how reducing the number of training samples or increasing model complexity (e.g., more layers or neurons) affects performance on the **test dataset**.  \n",
        "   ➤ Can you spot when the model starts to overfit?\n",
        "\n",
        "2. **Try different techniques to prevent overfitting (overtraining):**  \n",
        "   - Use **dropout** in the hidden layers.  \n",
        "   - Apply **weight regularization** (L1 or L2).  \n",
        "   - Apply **early stopping**.\n",
        "   - Reduce the size of the model (fewer layers or neurons).  \n",
        "   ➤ Which of these techniques improve generalization? Try combining them."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Early stopping\n",
        "from keras.callbacks import EarlyStopping\n",
        "early_stopping = EarlyStopping(monitor=\"val_loss\", patience=3, restore_best_weights=True)"
      ],
      "metadata": {
        "id": "baHVEZuC_xhR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Regularization:\n",
        "regularization_param = 0.0001\n",
        "kernel_regularizer = keras.regularizers.l2(regularization_param)\n",
        "# kernel_regularizer = keras.regularizers.l1(regularization_param)\n",
        "\n",
        "from keras import layers\n",
        "model1 = keras.Sequential([\n",
        "    layers.InputLayer(shape=(28 * 28,)),    # Input layer\n",
        "    layers.Dense(num_neur_1, activation=hidden_activation, kernel_regularizer=kernel_regularizer),\n",
        "    layers.Dense(num_neur_2, activation=hidden_activation, kernel_regularizer=kernel_regularizer),\n",
        "    layers.Dense(10, activation='softmax')  # Output layer for multiclass classification\n",
        "])\n",
        "model1.summary()\n",
        "#model = model1"
      ],
      "metadata": {
        "id": "T4hP98e-_u-4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dropout layers:\n",
        "dropout_rate = 0.1\n",
        "\n",
        "model1 = keras.Sequential([\n",
        "    keras.layers.InputLayer(shape=(28 * 28,)),\n",
        "    keras.layers.Dense(num_neur_1, activation=hidden_activation),\n",
        "    keras.layers.Dropout(dropout_rate),  # Add after each dense layer\n",
        "    keras.layers.Dense(num_neur_2, activation=hidden_activation),\n",
        "    keras.layers.Dropout(dropout_rate),\n",
        "    keras.layers.Dense(10, activation='softmax')  # output layer\n",
        "    ])\n",
        "\n",
        "model1.summary()\n",
        "#model = model1\n",
        "\n"
      ],
      "metadata": {
        "id": "EKWESm5e_8gC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Solution of some exercises"
      ],
      "metadata": {
        "id": "W2cFHcy5D0wV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Early stopping"
      ],
      "metadata": {
        "id": "KZ1CJmVkJT4I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set some of the hyperparameters:\n",
        "do_early_stopping = True\n",
        "max_epochs = 50\n",
        "batch_size = 128\n",
        "num_neur_1 = 512 # number of neurons in the first hidden layer\n",
        "num_neur_2 = 256 # number of neurons in the second hidden layer\n",
        "hidden_activation = 'relu'\n",
        "do_tensorboard = True\n",
        "details = \"\" # your comment\n",
        "# already set: do_tanh, do_one_hot_encoding\n",
        "# further hyperparameters: optimizer, loss, metrics,...\n",
        "\n",
        "###############################################\n",
        "# Define the model architecture\n",
        "from keras import layers\n",
        "model = keras.Sequential([\n",
        "    layers.InputLayer(shape=(28 * 28,)),    # Input layer\n",
        "    layers.Dense(num_neur_1, activation=hidden_activation), # First hidden layer   # , kernel_initializer='he_normal', bias_initializer='zeros')\n",
        "    layers.Dense(num_neur_2, activation=hidden_activation), # Second hidden layer\n",
        "    # layers.Dense(num_neur_3, activation=hidden_activation), # Third hidden layer\n",
        "    layers.Dense(10, activation='softmax')  # Output layer for multiclass classification\n",
        "])\n",
        "import datetime\n",
        "model_name = \"mnist_mlp_\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") + '.keras'\n",
        "model.summary()\n",
        "\n",
        "# Configure the model:\n",
        "optimizer = keras.optimizers.Adam() #SGD(learning_rate = 0.1) # SGD, Adam, RMSProp\n",
        "if do_one_hot_encoding:\n",
        "    model.compile(optimizer=optimizer,\n",
        "                  loss= keras.losses.CategoricalCrossentropy(),\n",
        "                  metrics= [keras.metrics.CategoricalAccuracy(\"accuracy\")])\n",
        "else:\n",
        "    model.compile(optimizer=optimizer,\n",
        "                  loss= keras.losses.SparseCategoricalCrossentropy(),\n",
        "                  metrics= [keras.metrics.SparseCategoricalAccuracy(\"accuracy\")])\n",
        "\n",
        "###############################################\n",
        "# Define callbacks (e.g., early stopping):\n",
        "callbacks = []\n",
        "if do_early_stopping:\n",
        "    from keras.callbacks import EarlyStopping\n",
        "    early_stopping = EarlyStopping(monitor=\"val_loss\", patience=3, restore_best_weights=True)\n",
        "    callbacks.append(early_stopping)\n",
        "if do_tensorboard:\n",
        "    from keras.callbacks import TensorBoard\n",
        "    tensorboard_callback = TensorBoard(log_dir=\"./logs_mnist/\"+model_name, histogram_freq=1, write_steps_per_second=True)\n",
        "    callbacks.append(tensorboard_callback)\n",
        "\n",
        "################################################\n",
        "# Train the model\n",
        "import time\n",
        "start_time = time.time()\n",
        "history = model.fit(x_train, y_train, epochs=max_epochs, batch_size=batch_size, validation_data=(x_val, y_val),\n",
        "                    callbacks=callbacks)\n",
        "time_fit = time.time() - start_time\n",
        "\n",
        "###############################\n",
        "# Plot the training progress:\n",
        "plot_history(history)\n",
        "\n",
        "# Evaluate the model on the training, validation and test sets\n",
        "train_loss, train_acc = model.evaluate(x_train, y_train)\n",
        "val_loss, val_acc = model.evaluate(x_val, y_val)\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "\n",
        "print('Training accuracy:', train_acc, '\\nTrain loss:', train_loss)\n",
        "print('Validation accuracy:', val_acc, '\\nVal loss:', val_loss)\n",
        "print('Test accuracy:', test_acc, '\\nTest loss:', test_loss)\n",
        "\n",
        "###############################\n",
        "# Save the model:\n",
        "import os\n",
        "model_dir = \"./models/\"\n",
        "if not os.path.exists(model_dir):\n",
        "    os.makedirs(model_dir)\n",
        "model.save(model_dir + model_name)\n",
        "\n",
        "#################################\n",
        "# Add results to the dataframe:\n",
        "model_details = f\"{num_neur_1}-{num_neur_2}-{hidden_activation}-{do_early_stopping}-ep.:{max_epochs}-bs:{batch_size} {details}\"\n",
        "new_entry = {\n",
        "    \"Model Name\" : model_name,\n",
        "    \"Details\" : model_details,\n",
        "    \"Test Accuracy\" : test_acc,\n",
        "    \"Test Loss\" : test_loss,\n",
        "    \"Train Accuracy\" : train_acc,\n",
        "    \"Train Loss\" : train_loss,\n",
        "    \"Time (s)\" : time_fit,\n",
        "    \"Epochs\" : len(history.epoch),\n",
        "}\n",
        "results_df = pd.concat([results_df, pd.DataFrame([new_entry])], ignore_index=True)\n",
        "# View and and save the dataframe:\n",
        "results_df.to_csv(model_dir + \"mnist_results.csv\", index=False)\n",
        "print(\"Results:\")\n",
        "print(results_df)"
      ],
      "metadata": {
        "id": "jI7_hqTZJYpg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_df"
      ],
      "metadata": {
        "id": "GKqIH85YJkFs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Regularization"
      ],
      "metadata": {
        "id": "LtKmdxlhEZhP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set some of the hyperparameters:\n",
        "regularization_param = 0.001\n",
        "kernel_regularizer = keras.regularizers.l2(regularization_param)\n",
        "\n",
        "do_early_stopping = True\n",
        "max_epochs = 100\n",
        "batch_size = 128\n",
        "num_neur_1 = 512 # number of neurons in the first hidden layer\n",
        "num_neur_2 = 256 # number of neurons in the second hidden layer\n",
        "hidden_activation = 'relu'\n",
        "do_tensorboard = True\n",
        "details = \"\" # your comment\n",
        "# already set: do_tanh, do_one_hot_encoding\n",
        "# further hyperparameters: optimizer, loss, metrics,...\n",
        "\n",
        "###############################################\n",
        "# Define the model architecture\n",
        "from keras import layers\n",
        "model = keras.Sequential([\n",
        "    layers.InputLayer(shape=(28 * 28,)),    # Input layer\n",
        "    layers.Dense(num_neur_1, activation=hidden_activation, kernel_regularizer=kernel_regularizer),\n",
        "    layers.Dense(num_neur_2, activation=hidden_activation, kernel_regularizer=kernel_regularizer),\n",
        "    layers.Dense(10, activation='softmax')  # Output layer for multiclass classification\n",
        "])\n",
        "import datetime\n",
        "model_name = \"mnist_mlp_\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") + '.keras'\n",
        "model.summary()\n",
        "\n",
        "# Configure the model:\n",
        "optimizer = keras.optimizers.Adam() #(learning_rate = 0.1) # SGD, Adam, RMSProp\n",
        "if do_one_hot_encoding:\n",
        "    model.compile(optimizer=optimizer,\n",
        "                  loss= keras.losses.CategoricalCrossentropy(),\n",
        "                  metrics= [keras.metrics.CategoricalAccuracy(\"accuracy\")])\n",
        "else:\n",
        "    model.compile(optimizer=optimizer,\n",
        "                  loss= keras.losses.SparseCategoricalCrossentropy(),\n",
        "                  metrics= [keras.metrics.SparseCategoricalAccuracy(\"accuracy\")])\n",
        "\n",
        "###############################################\n",
        "# Define callbacks (e.g., early stopping):\n",
        "callbacks = []\n",
        "if do_early_stopping:\n",
        "    from keras.callbacks import EarlyStopping\n",
        "    early_stopping = EarlyStopping(monitor=\"val_loss\", patience=10, restore_best_weights=True)\n",
        "    callbacks.append(early_stopping)\n",
        "if do_tensorboard:\n",
        "    from keras.callbacks import TensorBoard\n",
        "    tensorboard_callback = TensorBoard(log_dir=\"./logs_mnist/\"+model_name, histogram_freq=1, write_steps_per_second=True)\n",
        "    callbacks.append(tensorboard_callback)\n",
        "\n",
        "################################################\n",
        "# Train the model\n",
        "import time\n",
        "start_time = time.time()\n",
        "history = model.fit(x_train, y_train, epochs=max_epochs, batch_size=batch_size, validation_data=(x_val, y_val),\n",
        "                    callbacks=callbacks)\n",
        "time_fit = time.time() - start_time\n",
        "\n",
        "###############################\n",
        "# Plot the training progress:\n",
        "plot_history(history)\n",
        "\n",
        "# Evaluate the model on the training, validation and test sets\n",
        "train_loss, train_acc = model.evaluate(x_train, y_train)\n",
        "val_loss, val_acc = model.evaluate(x_val, y_val)\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "\n",
        "print('Training accuracy:', train_acc, '\\nTrain loss:', train_loss)\n",
        "print('Validation accuracy:', val_acc, '\\nVal loss:', val_loss)\n",
        "print('Test accuracy:', test_acc, '\\nTest loss:', test_loss)\n",
        "\n",
        "###############################\n",
        "# Save the model:\n",
        "import os\n",
        "model_dir = \"./models/\"\n",
        "if not os.path.exists(model_dir):\n",
        "    os.makedirs(model_dir)\n",
        "model.save(model_dir + model_name)\n",
        "\n",
        "#################################\n",
        "# Add results to the dataframe:\n",
        "model_details = f\"{num_neur_1}-{num_neur_2}-{hidden_activation}-{do_early_stopping}-ep.:{max_epochs}-bs:{batch_size} {details}\"\n",
        "new_entry = {\n",
        "    \"Model Name\" : model_name,\n",
        "    \"Details\" : model_details,\n",
        "    \"Test Accuracy\" : test_acc,\n",
        "    \"Test Loss\" : test_loss,\n",
        "    \"Train Accuracy\" : train_acc,\n",
        "    \"Train Loss\" : train_loss,\n",
        "    \"Time (s)\" : time_fit,\n",
        "    \"Epochs\" : len(history.epoch),\n",
        "}\n",
        "results_df = pd.concat([results_df, pd.DataFrame([new_entry])], ignore_index=True)\n",
        "# View and and save the dataframe:\n",
        "results_df.to_csv(model_dir + \"mnist_results.csv\", index=False)\n",
        "print(\"Results:\")\n",
        "print(results_df)"
      ],
      "metadata": {
        "id": "8CNZ0zouEVE3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_df"
      ],
      "metadata": {
        "id": "Cxc6lAIQEg4A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dropout"
      ],
      "metadata": {
        "id": "Y-T3ngFtD7rQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set some of the hyperparameters:\n",
        "dropout_rate = 0.6\n",
        "\n",
        "do_early_stopping = True\n",
        "max_epochs = 100\n",
        "batch_size = 128\n",
        "num_neur_1 = 512 # number of neurons in the first hidden layer\n",
        "num_neur_2 = 256 # number of neurons in the second hidden layer\n",
        "hidden_activation = 'relu'\n",
        "do_tensorboard = True\n",
        "details = \"\" # your comment\n",
        "# already set: do_tanh, do_one_hot_encoding\n",
        "# further hyperparameters: optimizer, loss, metrics,...\n",
        "\n",
        "###############################################\n",
        "# Define the model architecture\n",
        "from keras import layers\n",
        "model = keras.Sequential([\n",
        "    keras.layers.InputLayer(shape=(28 * 28,)),\n",
        "    keras.layers.Dense(num_neur_1, activation=hidden_activation),\n",
        "    keras.layers.Dropout(dropout_rate),  # Add after each dense layer\n",
        "    keras.layers.Dense(num_neur_2, activation=hidden_activation),\n",
        "    keras.layers.Dropout(dropout_rate),\n",
        "    keras.layers.Dense(10, activation='softmax')  # output layer\n",
        "    ])\n",
        "\n",
        "import datetime\n",
        "model_name = \"mnist_mlp_\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") + '.keras'\n",
        "model.summary()\n",
        "\n",
        "# Configure the model:\n",
        "optimizer = keras.optimizers.Adam() #SGD(learning_rate = 0.1) # SGD, Adam, RMSProp\n",
        "if do_one_hot_encoding:\n",
        "    model.compile(optimizer=optimizer,\n",
        "                  loss= keras.losses.CategoricalCrossentropy(),\n",
        "                  metrics= [keras.metrics.CategoricalAccuracy(\"accuracy\")])\n",
        "else:\n",
        "    model.compile(optimizer=optimizer,\n",
        "                  loss= keras.losses.SparseCategoricalCrossentropy(),\n",
        "                  metrics= [keras.metrics.SparseCategoricalAccuracy(\"accuracy\")])\n",
        "\n",
        "###############################################\n",
        "# Define callbacks (e.g., early stopping):\n",
        "callbacks = []\n",
        "if do_early_stopping:\n",
        "    from keras.callbacks import EarlyStopping\n",
        "    early_stopping = EarlyStopping(monitor=\"val_loss\", patience=3, restore_best_weights=True)\n",
        "    callbacks.append(early_stopping)\n",
        "if do_tensorboard:\n",
        "    from keras.callbacks import TensorBoard\n",
        "    tensorboard_callback = TensorBoard(log_dir=\"./logs_mnist/\"+model_name, histogram_freq=1, write_steps_per_second=True)\n",
        "    callbacks.append(tensorboard_callback)\n",
        "\n",
        "################################################\n",
        "# Train the model\n",
        "import time\n",
        "start_time = time.time()\n",
        "history = model.fit(x_train, y_train, epochs=max_epochs, batch_size=batch_size, validation_data=(x_val, y_val),\n",
        "                    callbacks=callbacks)\n",
        "time_fit = time.time() - start_time\n",
        "\n",
        "###############################\n",
        "# Plot the training progress:\n",
        "plot_history(history)\n",
        "\n",
        "# Evaluate the model on the training, validation and test sets\n",
        "train_loss, train_acc = model.evaluate(x_train, y_train)\n",
        "val_loss, val_acc = model.evaluate(x_val, y_val)\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "\n",
        "print('Training accuracy:', train_acc, '\\nTrain loss:', train_loss)\n",
        "print('Validation accuracy:', val_acc, '\\nVal loss:', val_loss)\n",
        "print('Test accuracy:', test_acc, '\\nTest loss:', test_loss)\n",
        "\n",
        "###############################\n",
        "# Save the model:\n",
        "import os\n",
        "model_dir = \"./models/\"\n",
        "if not os.path.exists(model_dir):\n",
        "    os.makedirs(model_dir)\n",
        "model.save(model_dir + model_name)\n",
        "\n",
        "#################################\n",
        "# Add results to the dataframe:\n",
        "model_details = f\"{num_neur_1}-{num_neur_2}-{hidden_activation}-{do_early_stopping}-ep.:{max_epochs}-bs:{batch_size} {details}\"\n",
        "new_entry = {\n",
        "    \"Model Name\" : model_name,\n",
        "    \"Details\" : model_details,\n",
        "    \"Test Accuracy\" : test_acc,\n",
        "    \"Test Loss\" : test_loss,\n",
        "    \"Train Accuracy\" : train_acc,\n",
        "    \"Train Loss\" : train_loss,\n",
        "    \"Time (s)\" : time_fit,\n",
        "    \"Epochs\" : len(history.epoch),\n",
        "}\n",
        "results_df = pd.concat([results_df, pd.DataFrame([new_entry])], ignore_index=True)\n",
        "# View and and save the dataframe:\n",
        "results_df.to_csv(model_dir + \"mnist_results.csv\", index=False)\n",
        "print(\"Results:\")\n",
        "print(results_df)"
      ],
      "metadata": {
        "id": "dRN5AlevDM4N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_df"
      ],
      "metadata": {
        "id": "UqHGU5vdEKY9"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}